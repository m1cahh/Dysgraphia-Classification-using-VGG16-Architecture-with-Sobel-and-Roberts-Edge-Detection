{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"ob7ZilaKjAea","executionInfo":{"status":"ok","timestamp":1734424093579,"user_tz":-480,"elapsed":11,"user":{"displayName":"MARIANILETTE BARTE","userId":"05980561890292952301"}}},"outputs":[],"source":["# Functions\n","def pad_and_resize(image, target_size=(224, 224)):\n","    width, height = image.size\n","    longer_side = max(width, height)\n","    horizontal_padding = (longer_side - width) // 2\n","    vertical_padding = (longer_side - height) // 2\n","\n","    padded_image = Image.new(image.mode, (longer_side, longer_side), color='black')\n","    padded_image.paste(image, (horizontal_padding, vertical_padding))\n","\n","    # Resize the padded image to the target size with anti-aliasing\n","    padded_image_resized = padded_image.resize(target_size)\n","\n","    return padded_image_resized\n","\n","\n","class RGBToGrayscaleLayer(keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","    def call(self, inputs):\n","        return tf.image.rgb_to_grayscale(inputs)\n","\n","\n","class GrayscaleToRGBLayer(keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","    def call(self, inputs):\n","        # Ensure input has a channel dimension\n","        if inputs.shape.rank == 3:  # Shape: (None, 224, 224)\n","            inputs = tf.expand_dims(inputs, axis=-1)  # Shape: (None, 224, 224, 1)\n","        return tf.image.grayscale_to_rgb(inputs)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[:-1] + (3,)  # Output has 3 channels\n","\n","# Define ImageEnhancementLayer\n","class ImageEnhancementLayer(keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","    def call(self, inputs):\n","        if tf.executing_eagerly():\n","            # PIL-based enhancements (for debugging and visualization)\n","            img = tf.keras.preprocessing.image.array_to_img(inputs[0])  # Access first image in batch\n","            brightness_factor = 1 + (34 / 100)\n","            x_enhanced = ImageEnhance.Brightness(img).enhance(brightness_factor)\n","            contrast_factor = 1 + (30 / 100)\n","            x_enhanced = ImageEnhance.Contrast(x_enhanced).enhance(contrast_factor)\n","            sharpness_factor = 1 + (70 / 100)\n","            x_enhanced = ImageEnhance.Sharpness(x_enhanced).enhance(sharpness_factor)\n","            x_enhanced = x_enhanced.filter(ImageFilter.MedianFilter(size=1))\n","\n","            # Convert back to tensor\n","            x_enhanced = tf.keras.preprocessing.image.img_to_array(x_enhanced)\n","            x_enhanced = tf.expand_dims(x_enhanced, 0)  # Restore batch dimension\n","            x_enhanced = tf.cast(x_enhanced, inputs.dtype)\n","        else:\n","            # Placeholder for TensorFlow-based enhancements during training\n","            x_enhanced = inputs\n","        return x_enhanced\n","\n","class StrokeWidthLayer(layers.Layer):\n","    def call(self, sobel_output):\n","        # Ensure the input is from the SobelLayer\n","        if sobel_output.shape[-1] != 1:\n","            raise ValueError(\"Input must be the output of SobelLayer with 1 channel.\")\n","\n","        # Calculate the stroke width from the Sobel output\n","        # Here, we can compute the average or apply some additional processing\n","        stroke_width = tf.reduce_mean(sobel_output, axis=-1, keepdims=True)  # Example calculation\n","\n","        # Normalize the stroke width\n","        normalized_stroke_width = stroke_width / (tf.reduce_max(stroke_width) + 1e-8)\n","\n","        return normalized_stroke_width  # Output shape: (batch_size, height, width, 1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape  # Ensure output shape matches input shape\n","\n","# Define SobelLayer\n","class SobelLayer(keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(SobelLayer, self).__init__(**kwargs)\n","\n","        # Define Sobel kernels as constants\n","        self.vertical_kernel = tf.constant([[-1.0, 0.0, 1.0],\n","                                            [-2.0, 0.0, 2.0],\n","                                            [-1.0, 0.0, 1.0]], dtype=tf.float32)\n","\n","        self.horizontal_kernel = tf.constant([[1.0, 2.0, 1.0],\n","                                              [0.0, 0.0, 0.0],\n","                                              [-1.0, -2.0, -1.0]], dtype=tf.float32)\n","\n","        # Reshape the kernels to be 4D tensors (height, width, input channels, output channels)\n","        self.vertical_kernel = self.vertical_kernel[:, :, tf.newaxis, tf.newaxis]\n","        self.horizontal_kernel = self.horizontal_kernel[:, :, tf.newaxis, tf.newaxis]\n","\n","    def call(self, inputs):\n","        # Ensure input is a grayscale image\n","        if inputs.shape[-1] != 1:\n","            raise ValueError(\"Input must be a grayscale image with 1 channel.\")\n","\n","        # Convert inputs to float32\n","        inputs_float32 = tf.cast(inputs, tf.float32)\n","\n","        # Apply vertical and horizontal Sobel filters using 2D convolution\n","        vertical_edges = tf.nn.conv2d(inputs_float32, self.vertical_kernel, strides=[1, 1, 1, 1], padding='SAME')\n","        horizontal_edges = tf.nn.conv2d(inputs_float32, self.horizontal_kernel, strides=[1, 1, 1, 1], padding='SAME')\n","\n","        # Compute the magnitude of the gradients (L2 norm of vertical and horizontal gradients)\n","        gradient_magnitude = tf.sqrt(tf.square(vertical_edges) + tf.square(horizontal_edges))\n","\n","        return tf.expand_dims(gradient_magnitude, axis=-1)  # Ensure output is (batch_size, height, width, 1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","\n","class RobertsLayer(layers.Layer):\n","    def call(self, inputs):\n","        # Convert inputs to float32\n","        inputs_float32 = tf.cast(inputs, tf.float32)\n","\n","        # Check the number of channels\n","        num_channels = inputs.shape[-1]\n","\n","        # Define Roberts Cross kernels\n","        kernel_x = tf.constant([[1, 0], [0, -1]], dtype=tf.float32)\n","        kernel_y = tf.constant([[0, 1], [-1, 0]], dtype=tf.float32)\n","\n","        # Reshape kernels to match the number of input channels\n","        kernel_x = tf.reshape(kernel_x, [2, 2, 1, 1])\n","        kernel_y = tf.reshape(kernel_y, [2, 2, 1, 1])\n","\n","        # Apply convolution using Roberts Cross kernels for each channel\n","        grad_x = [tf.nn.conv2d(inputs_float32[..., i:i+1], kernel_x, strides=[1, 1, 1, 1], padding='SAME') for i in range(num_channels)]\n","        grad_y = [tf.nn.conv2d(inputs_float32[..., i:i+1], kernel_y, strides=[1, 1, 1, 1], padding='SAME') for i in range(num_channels)]\n","\n","        # Concatenate the gradients along the channel dimension (if more than one channel)\n","        grad_x = tf.concat(grad_x, axis=-1)\n","        grad_y = tf.concat(grad_y, axis=-1)\n","\n","        # Compute the gradient magnitude\n","        grad_magnitude = tf.sqrt(tf.square(grad_x) + tf.square(grad_y))\n","\n","        # Normalize the gradient magnitudes\n","        grad_magnitude /= tf.reduce_max(grad_magnitude)\n","\n","        # Return the result with the same number of channels as input\n","        return grad_magnitude\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","class MorphologicalLayer(layers.Layer):\n","    def __init__(self, kernel_size=(2, 2), **kwargs):\n","        super(MorphologicalLayer, self).__init__(**kwargs)\n","        self.kernel_size = kernel_size\n","\n","    def call(self, inputs):\n","        # Ensure inputs have the shape (batch_size, height, width, 1) for grayscale\n","        if inputs.shape[-1] != 1:\n","            raise ValueError(\"Input must be a grayscale image with a single channel.\")\n","\n","        # Define the kernel for dilation and erosion\n","        kernel = tf.ones(tuple(self.kernel_size) + (1,), dtype=tf.float32)\n","\n","        # Perform dilation\n","        dilated_image = tf.image.extract_patches(\n","            images=inputs,\n","            sizes=[1, self.kernel_size[0], self.kernel_size[1], 1],\n","            strides=[1, 1, 1, 1],\n","            rates=[1, 1, 1, 1],\n","            padding='SAME'\n","        )\n","        dilated_image = tf.reduce_max(dilated_image, axis=-1, keepdims=True)\n","\n","        # Perform erosion\n","        eroded_image = tf.image.extract_patches(\n","            images=dilated_image,\n","            sizes=[1, self.kernel_size[0], self.kernel_size[1], 1],\n","            strides=[1, 1, 1, 1],\n","            rates=[1, 1, 1, 1],\n","            padding='SAME'\n","        )\n","        eroded_image = tf.reduce_min(eroded_image, axis=-1, keepdims=True)\n","        return eroded_image\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape  # Ensure output shape matches input shape\n","\n","\n","class SqueezeLayer(layers.Layer):\n","    def __init__(self, axis=-1, **kwargs):\n","        super(SqueezeLayer, self).__init__(**kwargs)\n","        self.axis = axis\n","\n","    def call(self, inputs):\n","        squeezed = tf.squeeze(inputs, axis=self.axis)  # Remove extra dimensions\n","\n","        # If squeezed shape has 3 dimensions and we have grayscale images, restore the channel dimension\n","        if len(squeezed.shape) == 3:  # (batch, height, width) for grayscale image\n","            restored = tf.expand_dims(squeezed, axis=-1)  # (batch_size, 224, 224, 1)\n","        else:\n","            restored = squeezed  # No expansion needed, already 4D\n","\n","        return restored\n","\n","\n","# Preprocessing function\n","def preprocess_image(img):\n","    img = pad_and_resize(img, target_size=image_size)\n","    img_array = np.array(img)\n","    if img_array.shape[-1] == 4:\n","        img_array = img_array[..., :3]\n","    img_array = np.expand_dims(img_array, 0) / 255.0\n","    return img_array\n","\n","# vgg16 full processing pipeline\n","def vgg16_processing_pipeline(img_array):\n","    x = ImageEnhancementLayer()(img_array)\n","    x = RGBToGrayscaleLayer()(x)\n","    x = StrokeWidthLayer()(x)\n","    x = SqueezeLayer(axis=-1)(x)\n","    final_output = GrayscaleToRGBLayer()(x)\n","\n","    return final_output\n","\n","# vgg16_roberts full processing pipeline\n","def roberts_processing_pipeline(img_array):\n","    x = RGBToGrayscaleLayer()(img_array)\n","    x = RobertsLayer()(x)\n","    x = StrokeWidthLayer()(x)\n","    x = SqueezeLayer(axis=-1)(x)\n","    x = MorphologicalLayer()(x)\n","    final_output = GrayscaleToRGBLayer()(x)\n","\n","    return final_output\n","\n","# vgg16_sobel full processing pipeline\n","def sobel_processing_pipeline(img_array):\n","    x = RGBToGrayscaleLayer()(img_array)\n","    x = SobelLayer()(x)\n","    x = StrokeWidthLayer()(x)\n","    x = SqueezeLayer(axis=-1)(x)\n","    x = MorphologicalLayer()(x)\n","    final_output = GrayscaleToRGBLayer()(x)\n","\n","    return final_output\n","\n","\n","def analyze_handwriting(img):\n","    try:\n","        # Convert image to grayscale for analysis\n","        img_gray = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)\n","\n","        # Apply adaptive thresholding to handle varying handwriting qualities\n","        binary_image = cv2.adaptiveThreshold(\n","            img_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2\n","        )\n","\n","        # Extract contours\n","        contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","        # Debug: Print the number of contours found\n","        print(f\"Number of contours found: {len(contours)}\")\n","\n","        if len(contours) == 0:\n","            return \"No contours found in the image.\"\n","\n","        # Extract bounding boxes\n","        bounding_boxes = [cv2.boundingRect(contour) for contour in contours]\n","\n","        # Filter out invalid bounding boxes (e.g., zero width or height)\n","        bounding_boxes = [b for b in bounding_boxes if b[2] > 0 and b[3] > 0]\n","\n","        # Debug: Print bounding boxes\n","        print(f\"Bounding boxes: {bounding_boxes}\")\n","\n","        if len(bounding_boxes) == 0:\n","            return \"No valid bounding boxes found.\"\n","\n","        # Calculate Inter-Character Spacing\n","        spacings = [\n","            bounding_boxes[i + 1][0] - (bounding_boxes[i][0] + bounding_boxes[i][2])\n","            for i in range(len(bounding_boxes) - 1)\n","        ]\n","        avg_spacing = np.mean(spacings) if spacings else 0\n","\n","        # Calculate Baseline Alignment\n","        baseline_y = [y + h for _, y, _, h in bounding_boxes]\n","        mean_baseline = np.mean(baseline_y) if baseline_y else 0\n","        baseline_deviations = [abs(y - mean_baseline) for y in baseline_y]\n","        avg_baseline_deviation = np.mean(baseline_deviations) if baseline_deviations else 0\n","\n","        # Calculate Character Size Variation\n","        heights = [h for _, _, _, h in bounding_boxes]\n","        size_std_dev = np.std(heights) if heights else 0\n","\n","        return avg_spacing, avg_baseline_deviation, size_std_dev\n","\n","    except Exception as e:\n","        return f\"Error in handwriting analysis: {e}\"\n","\n","\n","def label_criteria(value, thresholds, severity='spacing'):\n","    if severity == 'spacing':\n","        # Assuming thresholds is a list with [min_spacing, max_spacing]\n","        if value < thresholds[0]:\n","            return \"Severely Irregular Spacing\"\n","        else:\n","            return \"Normal to Slightly Irregular Spacing\"\n","\n","    elif severity == 'baseline':\n","        # Assuming thresholds is a single value for baseline threshold\n","        if value <= thresholds:\n","            return \"Normal to Mild Misalignment\"\n","        else:\n","            return \"Severe Misalignment\"\n","\n","    elif severity == 'size':\n","        # Assuming thresholds is a list with [min_size, max_size]\n","        if value <= thresholds:\n","            return \"Normal to Moderate Size Variation\"\n","        else:\n","            return \"Severe Size Variation\"\n","\n","    return \"Invalid Threshold\""]},{"cell_type":"code","source":["# Load the vgg16 model\n","with custom_object_scope({\n","    'ImageEnhancementLayer': ImageEnhancementLayer,\n","    'RGBToGrayscaleLayer': RGBToGrayscaleLayer,\n","    'StrokeWidthLayer': StrokeWidthLayer,\n","    'GrayscaleToRGBLayer': GrayscaleToRGBLayer}):\n","    vgg16_model = load_model(\"/content/drive/Shared drives/CS FILES/Model/vgg16_model.h5\")\n","\n","\n","# Load the vgg16_roberts model\n","with keras.utils.custom_object_scope({\n","    'RGBToGrayscaleLayer': RGBToGrayscaleLayer,\n","    'RobertsLayer': RobertsLayer,\n","    'StrokeWidthLayer': StrokeWidthLayer,\n","    'SqueezeLayer': SqueezeLayer,\n","    'MorphologicalLayer': MorphologicalLayer,\n","    'GrayscaleToRGBLayer': GrayscaleToRGBLayer\n","}):\n","    roberts_model = load_model(\"/content/drive/Shared drives/CS FILES/Model/vgg16_roberts_model.h5\")\n","\n","# Load the vgg16_sobel model\n","with keras.utils.custom_object_scope({\n","    'RGBToGrayscaleLayer': RGBToGrayscaleLayer,\n","    'SobelLayer': SobelLayer,\n","    'StrokeWidthLayer': StrokeWidthLayer,\n","    'SqueezeLayer': SqueezeLayer,\n","    'MorphologicalLayer': MorphologicalLayer,\n","    'GrayscaleToRGBLayer': GrayscaleToRGBLayer\n","}):\n","    sobel_model = load_model(\"/content/drive/Shared drives/CS FILES/Model/vgg16_sobel_model5.h5\")\n","\n","\n","\n","# Define the class labels\n","class_labels = [\"Low Potential Dysgraphia\", \"Potential Dysgraphia\"]\n","\n","# Diagnostic messages for each class\n","diagnostic_messages = {\n","    \"Low Potential Dysgraphia\": \"It suggests a minimal impairment in writing; however, observing difficulty in hand grip and positioning may still be beneficial.\",\n","    \"Potential Dysgraphia\": \"It is highly recommended to consult a specialist for further evaluation and support.\"\n","}\n","\n","# Define the image size (update based on your model's input size)\n","image_size = (224, 224)\n","\n","\n","# vgg16 prediction function\n","def vgg_predict_image(img):\n","    try:\n","        # Calculate handwriting metrics\n","        avg_spacing, avg_baseline_dev, size_std_dev = analyze_handwriting(img)\n","\n","        # Classify the raw values\n","        spacing_class = label_criteria(avg_spacing, [-125, 26], severity='spacing')\n","        baseline_class = label_criteria(avg_baseline_dev, [57], severity='baseline')\n","        size_class = label_criteria(size_std_dev, [51], severity='size')  # or [20, 50] for two thresholds\n","\n","         # Preprocess image\n","        img_array = preprocess_image(img)\n","\n","        # Process the image through the full pipeline\n","        final_processed_image = vgg16_processing_pipeline(img_array)\n","\n","        # Run model prediction\n","        predictions = vgg16_model.predict(img_array)\n","        predicted_class_index = np.argmax(predictions[0])\n","        class_label = class_labels[predicted_class_index]\n","        diagnostic_prompt = diagnostic_messages[class_label]\n","\n","        # Calculate percentages\n","        low_potential_percentage = predictions[0][0] * 100\n","        potential_percentage = predictions[0][1] * 100\n","        # Construct the output\n","        classification_text = (\n","            f'<div style=\"text-align: center;\">'\n","            f'<span style=\"font-size: 18px;\">This handwriting is <b>\"{class_label}\"</b></span><br>'\n","            f'<span style=\"font-size: 15px;\"><i>{diagnostic_prompt}</i></span><br><br>'\n","            f'<span style=\"font-size: 14px;\"><b>Confidence Level:</b></span><br>'\n","            f'<span style=\"font-size: 14px;\">({low_potential_percentage:.2f}% LPD, {potential_percentage:.2f}% PD)</span><br><br>'\n","            f'<span style=\"font-size: 14px;\"><b>VGG16 Model Accuracy:</b></span> 75.51%<br>'\n","            f'<span style=\"font-size: 14px;\"><b>VGG16 Model Precision:</b></span> 91.56%<br><br>'\n","            f'<span style=\"font-size: 15px;\"><b><u>Criteria:</u></b></span><br>'\n","            f'<span style=\"font-size: 14px;\"><b>• Inter-Character Spacing:</b></span> {spacing_class} ({avg_spacing:.2f})<br>'\n","            f'<span style=\"font-size: 14px;\"><b>• Baseline Alignment:</b></span> {baseline_class} ({avg_baseline_dev:.2f})<br>'\n","            f'<span style=\"font-size: 14px;\"><b>• Character Size Variation:</b></span> {size_class} ({size_std_dev:.2f})<br><br>'\n","            f'</div><br>'\n","        )\n","\n","\n","        return classification_text, final_processed_image\n","\n","    except Exception as e:\n","        return f\"Error in making prediction: {e}\"\n","\n","\n","# vgg16_roberts prediction function\n","def roberts_predict_image(img):\n","    try:\n","        # Calculate handwriting metrics\n","        avg_spacing, avg_baseline_dev, size_std_dev = analyze_handwriting(img)\n","\n","        # Classify the raw values\n","        spacing_class = label_criteria(avg_spacing, [-125, 26], severity='spacing')\n","        baseline_class = label_criteria(avg_baseline_dev, [57], severity='baseline')\n","        size_class = label_criteria(size_std_dev, [51], severity='size')  # or [20, 50] for two thresholds\n","\n","         # Preprocess image\n","        img_array = preprocess_image(img)\n","\n","        # Process the image through the full pipeline\n","        final_processed_image = roberts_processing_pipeline(img_array)\n","\n","        # Run model prediction\n","        predictions = roberts_model.predict(img_array)\n","        predicted_class_index = np.argmax(predictions[0])\n","        class_label = class_labels[predicted_class_index]\n","        diagnostic_prompt = diagnostic_messages[class_label]\n","\n","        # Calculate percentages\n","        low_potential_percentage = predictions[0][0] * 100\n","        potential_percentage = predictions[0][1] * 100\n","\n","        # Construct the output\n","        classification_text = (\n","            f'<div style=\"text-align: center;\">'\n","            f'<span style=\"font-size: 18px;\">This handwriting is <b>\"{class_label}\"</b></span><br>'\n","            f'<span style=\"font-size: 15px;\"><i>{diagnostic_prompt}</i></span><br><br>'\n","            f'<span style=\"font-size: 14px;\"><b>Confidence Level:</b></span><br>'\n","            f'<span style=\"font-size: 14px;\">({low_potential_percentage:.2f}% LPD, {potential_percentage:.2f}% PD)</span><br><br>'\n","            f'<span style=\"font-size: 14px;\"><b>VGG16 with Roberts Model Accuracy:</b> 81.88<br>'\n","            f'<span style=\"font-size: 14px;\"><b>VGG16 with Roberts Model Precision:</b> 93.59%<br><br>'\n","            f'<span style=\"font-size: 15px;\"><b><u>Criteria:</u></b></span><br>'\n","            f'<span style=\"font-size: 14px;\"><b>• Inter-Character Spacing:</b></span> {spacing_class} ({avg_spacing:.2f})<br>'\n","            f'<span style=\"font-size: 14px;\"><b>• Baseline Alignment:</b></span> {baseline_class} ({avg_baseline_dev:.2f})<br>'\n","            f'<span style=\"font-size: 14px;\"><b>• Character Size Variation:</b></span> {size_class} ({size_std_dev:.2f})<br><br>'\n","            f'</div><br>'\n","        )\n","\n","\n","        return classification_text, final_processed_image\n","\n","    except Exception as e:\n","        return f\"Error in making prediction: {e}\"\n","\n","\n","# vgg16_sobel prediction function\n","def sobel_predict_image(img):\n","    try:\n","        # Calculate handwriting metrics\n","        avg_spacing, avg_baseline_dev, size_std_dev = analyze_handwriting(img)\n","\n","        # Classify the raw values\n","        spacing_class = label_criteria(avg_spacing, [-125, 26], severity='spacing')\n","        baseline_class = label_criteria(avg_baseline_dev, [57], severity='baseline')\n","        size_class = label_criteria(size_std_dev, [51], severity='size')  # or [20, 50] for two thresholds\n","\n","         # Preprocess image\n","        img_array = preprocess_image(img)\n","\n","        # Process the image through the full pipeline\n","        final_processed_image = sobel_processing_pipeline(img_array)\n","\n","        # Run model prediction\n","        predictions = sobel_model.predict(img_array)\n","        predicted_class_index = np.argmax(predictions[0])\n","        class_label = class_labels[predicted_class_index]\n","        diagnostic_prompt = diagnostic_messages[class_label]\n","\n","        # Calculate percentages\n","        low_potential_percentage = predictions[0][0] * 100\n","        potential_percentage = predictions[0][1] * 100\n","\n","        # Construct the output\n","        classification_text = (\n","            f'<div style=\"text-align: center;\">'\n","            f'<span style=\"font-size: 18px;\">This handwriting is <b>\"{class_label}\"</b></span><br>'\n","            f'<span style=\"font-size: 15px;\"><i>{diagnostic_prompt}</i></span><br><br>'\n","            f'<span style=\"font-size: 14px;\"><b>Confidence Level:</b></span><br>'\n","            f'<span style=\"font-size: 14px;\">({low_potential_percentage:.2f}% LPD, {potential_percentage:.2f}% PD)</span><br><br>'\n","            f'<span style=\"font-size: 14px;\"><b>VGG16 with Sobel Model Accuracy:</b> 80.95%<br>'\n","            f'<span style=\"font-size: 14px;\"><b>VGG16 with Sobel Model Precision:</b> 93.06%<br><br>'\n","            f'<span style=\"font-size: 15px;\"><b><u>Criteria:</u></b></span><br>'\n","            f'<span style=\"font-size: 14px;\"><b>• Inter-Character Spacing:</b></span> {spacing_class} ({avg_spacing:.2f})<br>'\n","            f'<span style=\"font-size: 14px;\"><b>• Baseline Alignment:</b></span> {baseline_class} ({avg_baseline_dev:.2f})<br>'\n","            f'<span style=\"font-size: 14px;\"><b>• Character Size Variation:</b></span> {size_class} ({size_std_dev:.2f})<br><br>'\n","            f'</div><br>'\n","        )\n","\n","\n","        return classification_text, final_processed_image\n","\n","    except Exception as e:\n","        return f\"Error in making prediction: {e}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8H9LW1ijytQ","executionInfo":{"status":"ok","timestamp":1734424145676,"user_tz":-480,"elapsed":52105,"user":{"displayName":"MARIANILETTE BARTE","userId":"05980561890292952301"}},"outputId":"9b7a1c80-bfc7-4489-9772-4df193b86b31"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}]},{"cell_type":"code","source":["\n","# vgg16 gradio  interface\n","def vgg16_gradio_interface(image):\n","    vgg_classification_text, final_processed_image = vgg_predict_image(image)\n","    vgg_processed_image = Image.fromarray((tf.squeeze(final_processed_image).numpy() * 255).astype(np.uint8))\n","    return vgg_processed_image, vgg_classification_text\n","\n","# vgg16_roberts gradio  interface\n","def roberts_gradio_interface(image):\n","    roberts_classification_text, final_processed_image = roberts_predict_image(image)\n","    roberts_processed_image = Image.fromarray((tf.squeeze(final_processed_image).numpy() * 255).astype(np.uint8))\n","    return roberts_processed_image, roberts_classification_text\n","\n","# vgg16_sobel gradio  interface\n","def sobel_gradio_interface(image):\n","    sobel_classification_text, final_processed_image = sobel_predict_image(image)\n","    sobel_processed_image = Image.fromarray((tf.squeeze(final_processed_image).numpy() * 255).astype(np.uint8))\n","    return sobel_processed_image, sobel_classification_text\n","\n","\n","with gr.Blocks(fill_height=True) as demo:\n","    gr.HTML(\"<h1 style='text-align: center; font-size: 30x;'>Dysgraphia Classification using VGG16 Architecture with Sobel and Roberts Edge Detection in an Offline Handwriting Analysis</h1><h2 style='text-align: center; font-size: 20px;'>Choose a model then input an image to test if it's Low Potential or  Potential Dysgraphia</h2>\")\n","    with gr.Tab(\"vgg16\"):\n","        with gr.Row():\n","            with gr.Column():\n","                vgg_input_image = gr.Image(type=\"pil\", label=\"Input Image\")\n","                vgg16_button = gr.Button(\"Classify\")\n","            with gr.Column():\n","                vgg_processed_image = gr.Image(type=\"pil\", label=\"Processed Image\")\n","        with gr.Row():\n","            vgg_classification_text = gr.HTML(label=\"Classification Text\")\n","    with gr.Tab(\"vgg16_roberts\"):\n","        with gr.Row():\n","            with gr.Column():\n","                roberts_input_image = gr.Image(type=\"pil\", label=\"Input Image\")\n","                roberts_button = gr.Button(\"Classify\")\n","            with gr.Column():\n","                roberts_processed_image = gr.Image(type=\"pil\", label=\"Processed Image\")\n","        with gr.Row():\n","            roberts_classification_text = gr.HTML(label=\"Classification Text\")\n","    with gr.Tab(\"vgg16_sobel\"):\n","        with gr.Row():\n","            with gr.Column():\n","                sobel_input_image = gr.Image(type=\"pil\", label=\"Input Image\")\n","                sobel_button = gr.Button(\"Classify\")\n","            with gr.Column():\n","                sobel_processed_image = gr.Image(type=\"pil\", label=\"Processed Image\")\n","        with gr.Row():\n","            sobel_classification_text = gr.HTML(label=\"Classification Text\")\n","\n","    vgg16_button.click(\n","        fn=vgg16_gradio_interface,\n","        inputs=vgg_input_image,\n","        outputs=[vgg_processed_image, vgg_classification_text],\n","    )\n","    roberts_button.click(\n","        fn=roberts_gradio_interface,\n","        inputs=roberts_input_image,\n","        outputs=[roberts_processed_image, roberts_classification_text],\n","    )\n","    sobel_button.click(\n","        fn=sobel_gradio_interface,\n","        inputs=sobel_input_image,\n","        outputs=[sobel_processed_image, sobel_classification_text],\n","    )\n","\n","demo.launch(share=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":591},"id":"lNEZO8g6mCGl","executionInfo":{"status":"ok","timestamp":1734424148422,"user_tz":-480,"elapsed":2755,"user":{"displayName":"MARIANILETTE BARTE","userId":"05980561890292952301"}},"outputId":"d0761f9b-9e00-4b4f-9037-8bb08630b742"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://87ebea027abdd4f969.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://87ebea027abdd4f969.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":4}]}]}